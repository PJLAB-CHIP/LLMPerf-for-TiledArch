{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from gemm_tiling import gemm_tiling_input_stationary, gemm_tiling_weight_stationary\n",
    "import transformer_block as tbk\n",
    "import arch_execution as arch\n",
    "from util import *\n",
    "import math\n",
    "from mapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = load_config(\"./input/transformer/input0.json\")\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过tuning不同的tile_m，tile_n可以得到不同的性能结果\n",
    "\n",
    "因为 M=K=N，所以input stationary和output stationary 可以设置相反的tile_m和tile_n，从而得到完全一样的性能\n",
    "\n",
    "下面分析 q、k、v是否融合的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QKV projection, QKV=3, fuse q k v into one matrix\n",
      "QKV projection, M=4096, K=4096, N=12288, B=1, tile_m=64, tile_n=64, stationary: input, utilization=82.56%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[64, 64], Hardware utilization: 79.69%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "QKV projection, M=4096, K=4096, N=12288, B=1, tile_m=128, tile_n=32, stationary: input, utilization=91.84%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[128, 32], Hardware utilization: 83.23%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Warning: using defautl buffer strategy, nedd total memory 4.781250 MB, > SRAM 3.0 MB\n",
      "Warning: change input buffer strategy to input_buffer_num =  1\n",
      "QKV projection, M=4096, K=4096, N=12288, B=1, tile_m=256, tile_n=32, stationary: input, utilization=90.09%\n",
      "输入不复用则SRAM满足要求\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[256, 32], Hardware utilization: 79.35%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "QKV projection, M=4096, K=4096, N=12288, B=1, tile_m=32, tile_n=128, stationary: weight, utilization=83.36%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[32, 128], Hardware utilization: 0.00%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "# =============== QKV projection  ===================\n",
    "M, K, N = 4096, 4096, 4096\n",
    "QKV = 3   # QKV = 3, if fuse q k v into one matrix\n",
    "N = N * QKV\n",
    "details = False\n",
    "B = 1\n",
    "print(\"QKV projection, QKV=3, fuse q k v into one matrix\")\n",
    "tile_m = 64\n",
    "tile_n = 64\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = True\n",
    "ops[\"QKV_fusion\"] = model.gen_gemm(\n",
    "    \"QKV_fusion\", [B, model.config[\"S\"], model.config[\"H\"], model.config[\"H\"]])\n",
    "mapping_result['QKV_fusion'] = gemm_auto_opt_mapper(\n",
    "    ops['QKV_fusion'], hardware, Tm_Tn=Tm_Tn, fusion_op1=ops['RMSNorm'], details=details)\n",
    "utilization = mapping_result['QKV_fusion'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "\n",
    "tile_m = 128\n",
    "tile_n = 32\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = True\n",
    "ops[\"QKV_fusion\"] = model.gen_gemm(\n",
    "    \"QKV_fusion\", [B, model.config[\"S\"], model.config[\"H\"], model.config[\"H\"]])\n",
    "mapping_result['QKV_fusion'] = gemm_auto_opt_mapper(\n",
    "    ops['QKV_fusion'], hardware, Tm_Tn=Tm_Tn, fusion_op1=ops['RMSNorm'], details=details)\n",
    "utilization = mapping_result['QKV_fusion'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "\n",
    "tile_m = 256\n",
    "tile_n = 32\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = True\n",
    "ops[\"QKV_fusion\"] = model.gen_gemm(\n",
    "    \"QKV_fusion\", [B, model.config[\"S\"], model.config[\"H\"], model.config[\"H\"]])\n",
    "mapping_result['QKV_fusion'] = gemm_auto_opt_mapper(\n",
    "    ops['QKV_fusion'], hardware, Tm_Tn=Tm_Tn, fusion_op1=ops['RMSNorm'], details=details)\n",
    "utilization = mapping_result['QKV_fusion'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "\n",
    "tile_m = 32\n",
    "tile_n = 128\n",
    "utilization = gemm_tiling_weight_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: weight, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = False\n",
    "ops[\"QKV_fusion\"] = model.gen_gemm(\n",
    "    \"QKV_fusion\", [B, model.config[\"S\"], model.config[\"H\"], model.config[\"H\"]])\n",
    "mapping_result['QKV_fusion'] = gemm_auto_opt_mapper(\n",
    "    ops['QKV_fusion'], hardware, Tm_Tn=Tm_Tn, fusion_op1=ops['RMSNorm'], details=details)\n",
    "utilization = mapping_result['QKV_fusion'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，对于weight stationary来说，fuse qkv没太大变化，但是对于input stationary来说，fuse qkv会有很大的提升，因为提高了input 的复用率。\n",
    "\n",
    "下面分析batch size的影响"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 16\n",
      "QKV projection, batch size analysis, B=16\n",
      "QKV projection, M=4096, K=4096, N=4096, B=16, tile_m=64, tile_n=64, stationary: input, utilization=85.17%\n",
      "op input shape = [16, 4096, 4096]\n",
      "op output shape = [16, 4096, 4096]\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[64, 64], Hardware utilization: 5.20%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "QKV projection, M=4096, K=4096, N=4096, B=16, tile_m=128, tile_n=32, stationary: input, utilization=85.29%\n",
      "Batch size = 16\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[128, 32], Hardware utilization: 7.57%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Warning: using defautl buffer strategy, nedd total memory 4.781250 MB, > SRAM 3.0 MB\n",
      "Warning: change input buffer strategy to input_buffer_num =  1\n",
      "QKV projection, M=4096, K=4096, N=4096, B=16, tile_m=256, tile_n=32, stationary: input, utilization=84.89%\n",
      "Batch size = 16\n",
      "输入不复用则SRAM满足要求\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[256, 32], Hardware utilization: 11.05%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "QKV projection, M=4096, K=4096, N=4096, B=16, tile_m=32, tile_n=128, stationary: weight, utilization=98.36%\n",
      "Batch size = 16\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "Tm_Tn=[32, 128], Hardware utilization: 0.00%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "# =============== QKV projection  ===================\n",
    "M, K, N = 4096, 4096, 4096\n",
    "QKV = 1   # QKV = 3, if fuse q k v into one matrix\n",
    "N = N * QKV\n",
    "B = 16\n",
    "ops['K_proj']['ishape'][0] = B\n",
    "ops['K_proj']['oshape'][0] = B\n",
    "print(f\"Batch size = {ops['K_proj']['ishape'][0]}\")\n",
    "print(f\"QKV projection, batch size analysis, B={B}\")\n",
    "tile_m = 64\n",
    "tile_n = 64\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "ops['K_proj']['ishape'][0] = ops['K_proj']['ishape'][0]\n",
    "op = ops['K_proj']\n",
    "print(f\"op input shape = {op['ishape']}\")\n",
    "print(f\"op output shape = {op['oshape']}\")\n",
    "mapping_result['K_proj'] = gemm_auto_opt_mapper(\n",
    "    ops['K_proj'], hardware, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['K_proj'][\"utilization\"]*100\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "\n",
    "tile_m = 128\n",
    "tile_n = 32\n",
    "\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "print(f\"Batch size = {ops['K_proj']['ishape'][0]}\")\n",
    "\n",
    "mapping_result['K_proj'] = gemm_auto_opt_mapper(\n",
    "    ops['K_proj'], hardware, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['K_proj'][\"utilization\"]*100\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "\n",
    "tile_m = 256\n",
    "tile_n = 32\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "print(f\"Batch size = {ops['K_proj']['ishape'][0]}\")\n",
    "\n",
    "mapping_result['K_proj'] = gemm_auto_opt_mapper(\n",
    "    ops['K_proj'], hardware, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['K_proj'][\"utilization\"]*100\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "\n",
    "tile_m = 32\n",
    "tile_n = 128\n",
    "utilization = gemm_tiling_weight_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(\n",
    "    f\"QKV projection, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: weight, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "print(f\"Batch size = {ops['K_proj']['ishape'][0]}\")\n",
    "\n",
    "mapping_result['K_proj'] = gemm_auto_opt_mapper(\n",
    "    ops['K_proj'], hardware, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['K_proj'][\"utilization\"]*100\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

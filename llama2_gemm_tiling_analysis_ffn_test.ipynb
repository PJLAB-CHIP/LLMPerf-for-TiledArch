{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from gemm_tiling import gemm_tiling_input_stationary, gemm_tiling_weight_stationary\n",
    "import transformer_block as tbk\n",
    "import arch_execution as arch\n",
    "from util import *\n",
    "import math\n",
    "from mapper import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Up analysis, B=1\n",
      "FFN Up, M=4096, K=4096, N=11008, B=1, tile_m=64, tile_n=64, stationary: input, utilization=80.41%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "input Stationary, Tm_Tn=[64, 64], Hardware utilization: 80.39%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "# =============== FNN Up  ===================\n",
    "# FFN Gate 与 FFN Up是完全同样的size的计算，这里不列出了\n",
    "M, K, N = 4096, 4096, 11008\n",
    "details = False\n",
    "\n",
    "B = 1\n",
    "llm_config = {\n",
    "    \"B\": B,\n",
    "    \"S\": 4096,\n",
    "    \"H\": 4096,\n",
    "    \"A\": 32,\n",
    "    \"L\": 32,\n",
    "    \"H'\": 11008,\n",
    "    \"Q\": 16\n",
    "}\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}\n",
    "\n",
    "tile_m = 64\n",
    "tile_n = 64\n",
    "print(f\"FFN Up analysis, B={B}\")\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(f\"FFN Up, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "# [N/tile_n, B*M/tile_m] -- SRAM exceed -- debug shape info\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = True  # ishape <--> oshape, internal exchange shape\n",
    "\n",
    "mapping_result['FFNup'] = gemm_auto_opt_mapper(\n",
    "    ops['FFNup'], hardware, input_stationary=input_stationary, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['FFNup'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"{stationary} Stationary, Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 下面的代码段的结果有些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFN Up analysis, B=1\n",
      "FFN Up, M=4096, K=4096, N=11008, B=1, tile_m=32, tile_n=64, stationary: input, utilization=73.07%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "weight Stationary, Tm_Tn=[32, 64], Hardware utilization: 0.00%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "# =============== FNN Up  ===================\n",
    "# FFN Gate 与 FFN Up是完全同样的size的计算，这里不列出了\n",
    "M, K, N = 4096, 4096, 11008\n",
    "details = False\n",
    "\n",
    "B = 1\n",
    "llm_config = {\n",
    "    \"B\": B,\n",
    "    \"S\": 4096,\n",
    "    \"H\": 4096,\n",
    "    \"A\": 32,\n",
    "    \"L\": 32,\n",
    "    \"H'\": 11008,\n",
    "    \"Q\": 16\n",
    "}\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}\n",
    "\n",
    "tile_m, tile_n = 32, 64\n",
    "\n",
    "# 改成下面的，OK\n",
    "# tile_m, tile_n = 4, 86\n",
    "print(f\"FFN Up analysis, B={B}\")\n",
    "utilization = gemm_tiling_weight_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(f\"FFN Up, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "# [N/tile_n, B*M/tile_m] -- SRAM exceed -- debug shape info\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = False  # ishape <--> oshape, internal exchange shape\n",
    "\n",
    "mapping_result['FFNup'] = gemm_auto_opt_mapper(\n",
    "    ops['FFNup'], hardware, input_stationary=input_stationary, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['FFNup'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"{stationary} Stationary, Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== FNN Up  ===================\n",
    "# FFN Gate 与 FFN Up是完全同样的size的计算，这里不列出了\n",
    "M, K, N = 4096, 4096, 11008\n",
    "details = False\n",
    "\n",
    "\n",
    "B = 16\n",
    "llm_config = {\n",
    "    \"B\": B,\n",
    "    \"S\": 4096,\n",
    "    \"H\": 4096,\n",
    "    \"A\": 32,\n",
    "    \"L\": 32,\n",
    "    \"H'\": 11008,\n",
    "    \"Q\": 16\n",
    "}\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}\n",
    "\n",
    "tile_m = 4\n",
    "tile_n = 86\n",
    "print(f\"FFN Up analysis, B={B}\")\n",
    "utilization = gemm_tiling_weight_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(f\"FFN Up, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: input, utilization={utilization:.2f}%\")\n",
    "\n",
    "\n",
    "# [N/tile_n, B*M/tile_m] -- SRAM exceed -- debug shape info\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = False  # ishape <--> oshape, internal exchange shape\n",
    "\n",
    "mapping_result['FFNup'] = gemm_auto_opt_mapper(\n",
    "    ops['FFNup'], hardware, input_stationary=input_stationary, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['FFNup'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"{stationary} Stationary, Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============== FNN down  ===================\n",
    "M, K, N = 4096, 11008, 4096\n",
    "\n",
    "B = 1\n",
    "llm_config = {\n",
    "    \"B\": B,\n",
    "    \"S\": 4096,\n",
    "    \"H\": 4096,\n",
    "    \"A\": 32,\n",
    "    \"L\": 32,\n",
    "    \"H'\": 11008,\n",
    "    \"Q\": 16\n",
    "}\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}\n",
    "tile_m, tile_n = 4, 128\n",
    "utilization = gemm_tiling_input_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(f\"FFN Down, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: weight, utilization={utilization:.2f}%\")\n",
    "\n",
    "# [N/tile_n, B*M/tile_m] -- SRAM exceed -- debug shape info\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = True  # ishape <--> oshape, internal exchange shape\n",
    "\n",
    "mapping_result['FFN2'] = gemm_auto_opt_mapper(\n",
    "    ops['FFN2'], hardware, input_stationary=input_stationary, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['FFN2'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"{stationary} Stationary, Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: using defautl buffer strategy, nedd total memory 5.628906 MB, > SRAM 3.0 MB\n",
      "Warning: change input buffer strategy to input_buffer_num =  1\n",
      "FFN Down, M=4096, K=11008, N=4096, B=16, tile_m=4, tile_n=128, stationary: weight, utilization=98.23%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "weight Stationary, Tm_Tn=[4, 128], Hardware utilization: 98.24%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n"
     ]
    }
   ],
   "source": [
    "# =============== FNN down  ===================\n",
    "M, K, N = 4096, 11008, 4096\n",
    "\n",
    "B = 16\n",
    "llm_config = {\n",
    "    \"B\": B,\n",
    "    \"S\": 4096,\n",
    "    \"H\": 4096,\n",
    "    \"A\": 32,\n",
    "    \"L\": 32,\n",
    "    \"H'\": 11008,\n",
    "    \"Q\": 16\n",
    "}\n",
    "model = tbk.Llama_block(llm_config)\n",
    "tx8_config = load_config('hardware_parameter.json')\n",
    "hardware = arch.Tx8(tx8_config)\n",
    "Layers = model.config['L']\n",
    "ops = model.ops\n",
    "mapping_result = {}\n",
    "tile_m, tile_n = 4, 128\n",
    "utilization = gemm_tiling_weight_stationary(\n",
    "    B, M, K, N, tile_m, tile_n, print_details=False)\n",
    "print(f\"FFN Down, M={M}, K={K}, N={N}, B={B}, tile_m={tile_m}, tile_n={tile_n}, stationary: weight, utilization={utilization:.2f}%\")\n",
    "\n",
    "# [N/tile_n, B*M/tile_m] -- SRAM exceed -- debug shape info\n",
    "Tm_Tn = [int(tile_m), int(tile_n)]\n",
    "input_stationary = False  # ishape <--> oshape, internal exchange shape\n",
    "\n",
    "mapping_result['FFN2'] = gemm_auto_opt_mapper(\n",
    "    ops['FFN2'], hardware, input_stationary=input_stationary, Tm_Tn=Tm_Tn, details=details)\n",
    "utilization = mapping_result['FFN2'][\"utilization\"]*100\n",
    "stationary = \"input\" if input_stationary else \"weight\"\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")\n",
    "print(f\"{stationary} Stationary, Tm_Tn={Tm_Tn}, Hardware utilization: {utilization:.2f}%\")\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
